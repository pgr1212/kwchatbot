import os
import json
import psycopg2
import numpy as np
from openai import OpenAI
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from dotenv import load_dotenv

# ===== .env ë¶ˆëŸ¬ì˜¤ê¸° =====
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# ===== ê¸°ë³¸ ì„¤ì • =====
OPENAI_MODEL = "gpt-3.5-turbo"  # í•„ìš” ì‹œ gpt-4o ë“±ìœ¼ë¡œ ë³€ê²½ ê°€ëŠ¥
PG_DSN = {
    "host": "localhost",
    "dbname": "kwchatbot",  #  DB ì´ë¦„
    "user": "postgres",
    "password": "3864",  #  ë¹„ë°€ë²ˆí˜¸
}
SIM_THRESHOLD = 0.25  # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì„ê³„ê°’ (ë‚®ì„ìˆ˜ë¡ ë” ë§ì€ ê²°ê³¼ í—ˆìš©)

# ===== SentenceTransformer (ê³µê°œ í•œêµ­ì–´ SBERT ëª¨ë¸) =====
MODEL_PATH = "jhgan/ko-sbert-sts"
model = SentenceTransformer(MODEL_PATH)

# ===== OpenAI í´ë¼ì´ì–¸íŠ¸ =====
client = OpenAI(api_key=OPENAI_API_KEY)


# ==========================================
# ğŸ”¹ Postgresì—ì„œ top-3 ìœ ì‚¬ ì²­í¬ ê°€ì ¸ì˜¤ê¸°
# ==========================================
def _fetch_top3_chunks(query_embedding, categories):
    conn = psycopg2.connect(**PG_DSN)
    cur = conn.cursor()

    cur.execute(
        """
        SELECT dc.chunk_text, dc.chunk_metadata
        FROM embeddings e
        JOIN doc_chunks dc ON e.chunk_id = dc.chunk_id
        WHERE dc.chunk_metadata->>'ì¹´í…Œê³ ë¦¬' = ANY(%s)
        ORDER BY e.embedding <#> %s::vector
        LIMIT 3
        """,
        (categories, query_embedding),
    )
    cur.execute(
        """
        SELECT dc.chunk_text, dc.chunk_metadata
        FROM embeddings e
        JOIN doc_chunks dc ON e.chunk_id = dc.chunk_id
        WHERE dc.chunk_metadata->>'ì¹´í…Œê³ ë¦¬' = ANY(%s)
        ORDER BY e.embedding <#> %s::vector
        LIMIT 3
        """,
        (categories, query_embedding),
    )

    rows = cur.fetchall()

    #  ê²€ìƒ‰ëœ ì²­í¬ë¥¼ ì½˜ì†”ì— ì¶œë ¥
    #  ê²€ìƒ‰ëœ ì²­í¬ë¥¼ ì½˜ì†”ì— ì¶œë ¥
    print("\n========== ğŸ” ê²€ìƒ‰ëœ Top-3 ì²­í¬ ==========")
    if not rows:
        print("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        print("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        for i, (text, meta) in enumerate(rows, 1):
            category_name = (
                meta.get("ì¹´í…Œê³ ë¦¬", "ì—†ìŒ") if isinstance(meta, dict) else "ì—†ìŒ"
            )
            preview = text[:200].replace("\n", " ")  # ì¤„ë°”ê¿ˆ ì œê±° + ì¼ë¶€ë§Œ ì¶œë ¥
            print(f"[{i}] ì¹´í…Œê³ ë¦¬: {category_name}")
            print(f"ë³¸ë¬¸ ë¯¸ë¦¬ë³´ê¸°: {preview}...\n")
    print("=========================================\n")

    cur.close()
    conn.close()
    return rows


# ==========================================
# ğŸ”¹ í•µì‹¬: RAG ë‹µë³€ ìƒì„± í•¨ìˆ˜
# ==========================================
def generate_answer(user_query, category=None):
    # ì‚¬ìš©ì ì§ˆë¬¸ ì„ë² ë”© ìƒì„±
    query_embedding = model.encode(user_query).tolist()

    # í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ë°›ì€ ì¹´í…Œê³ ë¦¬ ì´ë¦„ ì •ë¦¬
    selected_category = None
    if category and isinstance(category, list):
        selected_category = category[0]

    # ë²„íŠ¼ ì´ë¦„ â†’ DB ì¹´í…Œê³ ë¦¬ ë§¤í•‘
    category_map = {
        "ê°•ì˜": ["ê°•ì˜ì •ë³´", "í•™ê³¼ì •ë³´"],
        "ë™ì•„ë¦¬": ["ë™ì•„ë¦¬"],
        "ì·¨ì—… ì •ë³´": ["ì·¨ì—…"],
    }
    db_categories = category_map.get(selected_category, None)

    # DBì—ì„œ ìœ ì‚¬í•œ ì²­í¬ top-3 ê²€ìƒ‰
    rows = _fetch_top3_chunks(query_embedding, categories=db_categories)

    # ìœ ì‚¬ë„ ì„ê³„ê°’ ê²€ì‚¬
    if not rows:
        return "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

    top_texts = [t for (t, _m) in rows]
    qvec = np.array(query_embedding).reshape(1, -1)
    cvecs = model.encode(top_texts)
    sims = cosine_similarity(qvec, cvecs)[0]

    if float(np.max(sims)) < SIM_THRESHOLD:
        return "í•´ë‹¹ ì£¼ì œì™€ ìœ ì‚¬í•œ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

    # ë¬¸ë§¥(context) êµ¬ì„±
    context_items = []
    for i, (text, meta) in enumerate(rows, 1):
        meta_str = (
            json.dumps(meta, ensure_ascii=False) if not isinstance(meta, str) else meta
        )
        context_items.append(f"-----\në³¸ë¬¸:\n{text}\në©”íƒ€ë°ì´í„°:\n{meta_str}")
    context = "\n".join(context_items)

    # LLM í˜¸ì¶œ - LLMì´ ë” ì˜ ë‹µë³€í•˜ë„ë¡ ì˜ì–´ë¡œ í”„ë¡¬í”„íŠ¸ ì‘ì„±
    system = """
You are the KW University Chatbot.
Answer the user's question ONLY based on the provided CONTEXT.
If the answer is not explicitly found in the CONTEXT, respond:
"ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
Do NOT use prior knowledge. Do NOT infer or guess.
Answer concisely and accurately in Korean.
"""

    user = f"""
CONTEXT:
{context}

USER QUESTION (in Korean):
{user_query}

INSTRUCTIONS:
1. Use only the content inside CONTEXT.
2. If information is missing, reply with:
   "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
3. Keep your answer clear and concise.
"""

    try:
        resp = client.chat.completions.create(
            model=OPENAI_MODEL,
            temperature=0.0,
            messages=[
                {"role": "system", "content": system},
                {"role": "user", "content": user},
            ],
        )
        return resp.choices[0].message.content

    except Exception as e:
        return f"LLM ì‘ë‹µ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
