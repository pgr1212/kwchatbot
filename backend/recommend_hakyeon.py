import os
import psycopg2
from sentence_transformers import SentenceTransformer
from dotenv import load_dotenv

load_dotenv()

PG_DSN = {
    "host": "localhost",
    "port": "5432",
    "dbname": "kwchatbot",
    "user": "postgres",
    "password": "3864"
}

MODEL_PATH = "jhgan/ko-sbert-sts"
try:
    model = SentenceTransformer(MODEL_PATH)
except:
    model = SentenceTransformer("jhgan/ko-sbert-sts")

def _fetch_similar_chunks(query_embedding, top_k=3):  # ë””ë²„ê¹…ì„ ìœ„í•´ ê¸°ë³¸ê°’ì„ 3ìœ¼ë¡œ ë³€ê²½
    conn = psycopg2.connect(**PG_DSN)
    cur = conn.cursor()
    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê²€ìƒ‰
    cur.execute(
        """
        SELECT dc.doc_id, dc.chunk_id, dc.chunk_text, dc.category, 1 - (e.embedding <#> %s::vector) AS similarity
        FROM embeddings e
        JOIN doc_chunks dc ON e.chunk_id = dc.chunk_id
        WHERE dc.category = 'ì—°êµ¬ì‹¤ ì •ë³´'
        ORDER BY e.embedding <#> %s::vector
        LIMIT %s;
        """,
        (query_embedding, query_embedding, top_k)
    )
    rows = cur.fetchall()
    cur.close()
    conn.close()
    return rows

def _fetch_all_chunks_by_doc(doc_id):
    conn = psycopg2.connect(**PG_DSN)
    cur = conn.cursor()
    cur.execute(
        """
        SELECT chunk_index, chunk_text FROM doc_chunks
        WHERE doc_id = %s ORDER BY chunk_index ASC;
        """,
        (doc_id,)
    )
    rows = cur.fetchall()
    cur.close()
    conn.close()
    return rows

def recommend_one_hakyeon(user_query):
    
    query_embedding = model.encode(user_query).tolist()
    
    # top_k=3ìœ¼ë¡œ ì—¬ëŸ¬ í›„ë³´ë¥¼ ê°€ì ¸ì™€ ë´…ë‹ˆë‹¤.
    rows = _fetch_similar_chunks(query_embedding, top_k=3)
    
    if not rows:
        print("[DEBUG] ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
        return None

    # 1ë“± í›„ë³´ ì„ íƒ
    best_doc_id = rows[0][0]
    best_sim = round(float(rows[0][4]), 4)
    
    # [ì¤‘ìš”] ìœ ì‚¬ë„ ì„ê³„ê°’ ì„¤ì • (ì˜ˆ: 0.3 ë¯¸ë§Œì´ë©´ ì¶”ì²œ ì•ˆ í•¨)
    # í•„ìš”í•˜ë‹¤ë©´ ì•„ë˜ ì£¼ì„ì„ í•´ì œí•´ì„œ ì‚¬ìš©í•˜ì„¸ìš”.
    # if best_sim < 0.35:
    #     print(f"[DEBUG] ìœ ì‚¬ë„({best_sim})ê°€ ë„ˆë¬´ ë‚®ì•„ ì¶”ì²œí•˜ì§€ ì•ŠìŒ")
    #     return None

    chunks = _fetch_all_chunks_by_doc(best_doc_id)
    if not chunks:
        return None

    full_text = "\n".join([txt for _, txt in chunks])
    lines = full_text.splitlines()

    # --- ğŸ” íŒŒì‹± ë¡œì§ ---
    name = "ì—°êµ¬ì‹¤ëª… ì •ë³´ ì—†ìŒ"
    professor = "êµìˆ˜ëª… ì •ë³´ ì—†ìŒ"
    field = "ì—°êµ¬ë¶„ì•¼ ì •ë³´ ì—†ìŒ"

    for line in lines:
        line_clean = line.strip()
        if not line_clean: continue

        if "ì—°êµ¬ì‹¤ëª…:" in line_clean or "ì—°êµ¬ì‹¤:" in line_clean:
             # ':' ë’¤ì— ë‚´ìš©ì´ ìˆìœ¼ë©´ ê°€ì ¸ì˜¤ê³  ì—†ìœ¼ë©´ ê±´ë„ˆëœ€
             parts = line_clean.split(":", 1)
             if len(parts) > 1 and parts[1].strip():
                 name = parts[1].strip()

        if "êµìˆ˜ëª…:" in line_clean or "ë‹´ë‹¹êµìˆ˜:" in line_clean:
             parts = line_clean.split(":", 1)
             if len(parts) > 1 and parts[1].strip():
                 professor = parts[1].strip()

        if "ì—°êµ¬ë¶„ì•¼:" in line_clean:
             parts = line_clean.split(":", 1)
             if len(parts) > 1 and parts[1].strip():
                 field = parts[1].strip()
        elif "ì—°êµ¬ë‚´ìš©:" in line_clean and field == "ì—°êµ¬ë¶„ì•¼ ì •ë³´ ì—†ìŒ":
             parts = line_clean.split(":", 1)
             if len(parts) > 1 and parts[1].strip():
                 field = parts[1].strip()

    # ì•ˆì „ì¥ì¹˜: íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì²« ì¤„ í™œìš©
    if name == "ì—°êµ¬ì‹¤ëª… ì •ë³´ ì—†ìŒ" and lines:
        first_line = lines[0].strip()
        if len(first_line) < 30 and ":" not in first_line:
            name = first_line

    return {
        "doc_id": best_doc_id,
        "name": name,
        "professor": professor,
        "field": field,
        "introduction": full_text,
        "score": best_sim
    }