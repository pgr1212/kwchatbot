# type: uploaded file
# fileName: backend/recommend_club.py
# ============================================
# ğŸ¤ SBERT ê¸°ë°˜ ë™ì•„ë¦¬ ì¶”ì²œ ì‹œìŠ¤í…œ (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ë²„ì „)
# ============================================

import os
import psycopg2
import numpy as np
from sentence_transformers import SentenceTransformer
from dotenv import load_dotenv

# ===== .env ë¶ˆëŸ¬ì˜¤ê¸° =====
load_dotenv()

# ===== ê¸°ë³¸ ì„¤ì • =====
PG_DSN = {
    "host": "localhost",
    "port": "5432",
    "dbname": "kwchatbot",   # âš ï¸ DB ì´ë¦„ í™•ì¸
    "user": "postgres",
    "password": "3864"       # âš ï¸ ë¹„ë°€ë²ˆí˜¸ í™•ì¸
}

# ===== SBERT ëª¨ë¸ =====
MODEL_PATH = "jhgan/ko-sbert-sts"
try:
    model = SentenceTransformer(MODEL_PATH)
except:
    model = SentenceTransformer("jhgan/ko-sbert-sts")

# ============================================
# ğŸ”¹ 1ï¸âƒ£ ë™ì•„ë¦¬ ì¹´í…Œê³ ë¦¬ì—ì„œ ìœ ì‚¬ ì²­í¬ ê²€ìƒ‰
# ============================================
def _fetch_similar_chunks(query_embedding, top_k=1):
    conn = psycopg2.connect(**PG_DSN)
    cur = conn.cursor()

    cur.execute(
        """
        SELECT 
            dc.doc_id,
            dc.chunk_id,
            dc.chunk_text,
            dc.category,
            (e.embedding <=> %s::vector) AS distance
        FROM embeddings e
        JOIN doc_chunks dc ON e.chunk_id = dc.chunk_id
        WHERE dc.category = 'ë™ì•„ë¦¬'
        ORDER BY e.embedding <=> %s::vector
        LIMIT %s;
        """,
        (query_embedding, query_embedding, top_k)
    )

    rows = cur.fetchall()
    cur.close()
    conn.close()
    return rows


# ============================================
# ğŸ”¹ 2ï¸âƒ£ ë™ì¼ ë™ì•„ë¦¬(doc_id)ì˜ ëª¨ë“  ì²­í¬ ê°€ì ¸ì˜¤ê¸°
# ============================================
def _fetch_all_chunks_by_doc(doc_id):
    conn = psycopg2.connect(**PG_DSN)
    cur = conn.cursor()

    cur.execute(
        """
        SELECT chunk_index, chunk_text
        FROM doc_chunks
        WHERE doc_id = %s
        ORDER BY chunk_index ASC;
        """,
        (doc_id,)
    )

    rows = cur.fetchall()
    cur.close()
    conn.close()
    return rows


# ============================================
# ğŸ”¹ 3ï¸âƒ£ ì¶”ì²œ í•¨ìˆ˜ (íŒŒì‹± ë¡œì§ ìˆ˜ì •: ì´ë¦„/ë¶„ì•¼/ì†Œì†)
# ============================================
def recommend_one_club(user_query):
    """
    ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ ë°›ì•„ ê°€ì¥ ì í•©í•œ ë™ì•„ë¦¬ 1ê°œë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤.
    ë°ì´í„° ë‚´ì—ì„œ 'ì´ë¦„', 'ë¶„ì•¼', 'ì†Œì†' í‚¤ì›Œë“œë¥¼ ì°¾ì•„ ì •ë³´ë¥¼ ë¶„ë¦¬í•©ë‹ˆë‹¤.
    """
    # 1. ì„ë² ë”© ìƒì„±
    query_embedding = model.encode(user_query).tolist()

    # 2. ìœ ì‚¬ ì²­í¬ ê²€ìƒ‰ (Top 1)
    rows = _fetch_similar_chunks(query_embedding, top_k=1)
    if not rows:
        return None

    # ê°€ì¥ ìœ ì‚¬í•œ ë™ì•„ë¦¬ ì„ íƒ
    best_doc_id = rows[0][0]
    best_sim = round(1 - float(rows[0][4]), 4)

    # 3. í•´ë‹¹ ë™ì•„ë¦¬ì˜ ì „ì²´ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
    chunks = _fetch_all_chunks_by_doc(best_doc_id)
    if not chunks:
        return None

    full_text = "\n".join([txt for _, txt in chunks])
    lines = full_text.splitlines()

    # --- ğŸ” íŒŒì‹± ë¡œì§ ê°•í™” ---
    name = "ë™ì•„ë¦¬ëª… ì •ë³´ ì—†ìŒ"
    field = "ë¶„ì•¼ ì •ë³´ ì—†ìŒ"
    affiliation = "ì†Œì† ì •ë³´ ì—†ìŒ" # ì†Œì† ì¶”ê°€

    # ëª¨ë“  ì¤„ì„ ê²€ì‚¬í•˜ì—¬ ì •ë³´ ì¶”ì¶œ
    for line in lines:
        line_clean = line.strip()
        
        # 1. ë™ì•„ë¦¬ëª… ì°¾ê¸° ('ì´ë¦„' í‚¤ì›Œë“œ)
        if "ì´ë¦„" in line_clean or "ë™ì•„ë¦¬ëª…" in line_clean:
            if ":" in line_clean:
                extracted_name = line_clean.split(":", 1)[1].strip()
                if extracted_name:
                    name = extracted_name
        
        # 2. ë¶„ì•¼ ì°¾ê¸° ('ë¶„ì•¼' í‚¤ì›Œë“œ)
        if "ë¶„ì•¼" in line_clean:
            if ":" in line_clean:
                extracted_field = line_clean.split(":", 1)[1].strip()
                if extracted_field:
                    field = extracted_field
        
        # 3. ì†Œì† ì°¾ê¸° ('ì†Œì†' í‚¤ì›Œë“œ) - ìƒˆë¡œ ì¶”ê°€ë¨
        if "ì†Œì†" in line_clean:
            if ":" in line_clean:
                extracted_affiliation = line_clean.split(":", 1)[1].strip()
                if extracted_affiliation:
                    affiliation = extracted_affiliation

    # í‚¤ì›Œë“œë¡œ ì´ë¦„ì„ ëª» ì°¾ì•˜ë‹¤ë©´, ì²« ì¤„ì„ ì œëª©ìœ¼ë¡œ ì‚¬ìš© (ì•ˆì „ì¥ì¹˜)
    if name == "ë™ì•„ë¦¬ëª… ì •ë³´ ì—†ìŒ" and lines:
        first_line = lines[0].strip()
        if "ë¶„ì•¼" not in first_line and "ì†Œì†" not in first_line and ":" not in first_line:
            name = first_line
    
    # 4. ê²°ê³¼ ë°˜í™˜
    # introductionì— full_textë¥¼ ë„£ì–´ì£¼ë©´ ëª¨ë‹¬ì—ì„œ ì „ì²´ ë°ì´í„°ë¥¼ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    return {
        "doc_id": best_doc_id,
        "name": name,             # ë™ì•„ë¦¬ ì´ë¦„
        "field": field,           # ë¶„ì•¼
        "affiliation": affiliation, # ì†Œì† (í”„ë¡ íŠ¸ì—”ë“œì— í‘œì‹œí•˜ë ¤ë©´ ì»´í¬ë„ŒíŠ¸ ìˆ˜ì • í•„ìš”í•  ìˆ˜ ìˆìŒ)
        "introduction": full_text,  # ìƒì„¸ ë‚´ìš© (ì „ì²´ ë°ì´í„°)
        "score": best_sim
    }

# ============================================
# ğŸ”¹ ì‹¤í–‰ ì˜ˆì‹œ
# ============================================
if __name__ == "__main__":
    query = "ë¡œë´‡ ë§Œë“œëŠ” ë™ì•„ë¦¬ ì¶”ì²œí•´ì¤˜"
    print(f"\n[ì‚¬ìš©ì ì…ë ¥] {query}\n")

    rec = recommend_one_club(query)

    if rec:
        print(f"ğŸ¯ ì¶”ì²œ ë™ì•„ë¦¬: {rec['name']} (ìœ ì‚¬ë„: {rec['score']})")
        print(f"ğŸ“‚ ë¶„ì•¼: {rec['field']}")
        print(f"ğŸ« ì†Œì†: {rec['affiliation']}")
        print(f"ğŸ“˜ ì „ì²´ ë°ì´í„°:\n{rec['introduction'][:100]}...")
    else:
        print("ì¶”ì²œ ê²°ê³¼ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")