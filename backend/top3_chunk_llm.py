import os
import json
import psycopg2
import numpy as np
from sentence_transformers import SentenceTransformer
from dotenv import load_dotenv
from openai import OpenAI

# ===== .env ë¶ˆëŸ¬ì˜¤ê¸° =====
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

client = OpenAI(api_key=OPENAI_API_KEY)

# ===== DB ì„¤ì • =====
PG_DSN = {
    "host": "localhost",
    "dbname": "kwchatbot_lec",
    "user": "postgres",
    "password": "3864"
}

# ===== SBERT ëª¨ë¸ =====
MODEL_PATH = "triplet_finetuned_model"
model = SentenceTransformer(MODEL_PATH)

# ============================================
# ğŸ”¹ ì¹´í…Œê³ ë¦¬ ë§¤í•‘
# ============================================
category_map = {
    "ê°•ì˜": ["ê°•ì˜ì •ë³´", "í•™ê³¼ì •ë³´"],
    "ë™ì•„ë¦¬": ["ë™ì•„ë¦¬"],
    "ë‚´ì¼ë°°ì›€": ["ë‚´ì¼ë°°ì›€"],
    "ì§ì—…ì •ë³´": ["ì§ì—…ì •ë³´"],
    "í•™ë¶€ì—°êµ¬ìƒ": ["ì—°êµ¬ì‹¤ ì •ë³´"]
}


# ============================================
# ğŸ”¹ 1ï¸âƒ£ íŠ¹ì • ì¹´í…Œê³ ë¦¬ ë‚´ì—ì„œ top3 ìœ ì‚¬ ì²­í¬ ê²€ìƒ‰
# ============================================
def fetch_top3_chunks(query_embedding, db_categories):
    conn = psycopg2.connect(**PG_DSN)
    cur = conn.cursor()

    cur.execute(
        """
        SELECT 
            dc.chunk_id,
            dc.doc_id,
            dc.chunk_index,
            dc.chunk_text,
            dc.category,
            dc.chunk_metadata,
            1 - (e.embedding <#> %s::vector) AS similarity
        FROM embeddings e
        JOIN doc_chunks dc ON e.chunk_id = dc.chunk_id
        WHERE dc.category = ANY(%s)
        ORDER BY e.embedding <#> %s::vector
        LIMIT 3;
        """,
        (query_embedding, db_categories, query_embedding)
    )

    rows = cur.fetchall()
    cur.close()
    conn.close()
    return rows


# ============================================
# ğŸ”¹ 2ï¸âƒ£ LLM ë‹µë³€ ìƒì„± (RAG)
# ============================================
def generate_llm_answer(user_query, chunks):
    context_items = []

    for text, meta in chunks:
        meta_str = json.dumps(meta, ensure_ascii=False)
        context_items.append(
            f"-----\në³¸ë¬¸:\n{text}\n\në©”íƒ€ë°ì´í„°:\n{meta_str}"
        )

    context = "\n".join(context_items)

    system_msg = """
ë‹¹ì‹ ì€ ê´‘ìš´ëŒ€í•™êµ KW Chatbotì…ë‹ˆë‹¤.
ì•„ë˜ ì œê³µëœ CONTEXT ì •ë³´ë§Œ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìê°€ ì•Œê¸°ì‰½ê²Œ ì¹œì ˆí•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
êµìˆ˜ëª…ë„ í•„ìš”í•˜ë©´ ê°™ì´ ëŒ€ë‹µí•˜ì„¸ìš”.

"""

    user_msg = f"{context}\n\nì§ˆë¬¸: {user_query}\n\nì •ë‹µ:"

    try:
        resp = client.chat.completions.create(
            model="gpt-3.5-turbo",
            temperature=0.0,
            messages=[
                {"role": "system", "content": system_msg},
                {"role": "user", "content": user_msg},
            ],
        )
        return resp.choices[0].message.content
    
    except Exception as e:
        return f"[LLM ì˜¤ë¥˜] {str(e)}"


# ============================================
# ğŸ”¹ 3ï¸âƒ£ Top-3 ì²­í¬ ì¶œë ¥ + LLM ë‹µë³€
# ============================================
def print_top3_and_llm(category_key, user_query):
    if category_key not in category_map:
        print("âŒ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì¹´í…Œê³ ë¦¬ì…ë‹ˆë‹¤.")
        return

    db_categories = category_map[category_key]
    query_embedding = model.encode(user_query).tolist()

    rows = fetch_top3_chunks(query_embedding, db_categories)

    if not rows:
        print("â— ìœ ì‚¬í•œ ì²­í¬ ì—†ìŒ")
        return

    # í”„ë¡¬í”„íŠ¸ìš© context ì¤€ë¹„
    llm_chunks = []

    print(f"\n========== ğŸ” Top-3 ìœ ì‚¬ ì²­í¬ (ì¹´í…Œê³ ë¦¬: {category_key}) ==========")
    for i, row in enumerate(rows, start=1):
        chunk_id, doc_id, idx, text, category, metadata, sim = row

        print(f"\n[{i}] â–¶ ì¹´í…Œê³ ë¦¬: {category}")
        print(f"   â–¸ doc_id: {doc_id}")
        print(f"   â–¸ chunk_id: {chunk_id}, index: {idx}")
        print(f"   â–¸ similarity: {round(float(sim), 4)}")

        print("\nğŸ“„ ì „ì²´ í…ìŠ¤íŠ¸:")
        print(text)

        print("\nğŸ—‚ ë©”íƒ€ë°ì´í„°:")
        print(json.dumps(metadata, ensure_ascii=False, indent=4))
        print("-" * 60)

        # LLMì— ë„˜ê¸¸ context êµ¬ì„±
        llm_chunks.append((text, metadata))

    print("\n========================================================\n")

    # ğŸ”¥ LLM ë‹µë³€ ìƒì„±
    llm_answer = generate_llm_answer(user_query, llm_chunks)

    print("\n================ ğŸ’¬ LLM ìµœì¢… ë‹µë³€ ================\n")
    print(llm_answer)
    print("\n==================================================\n")


# ============================================
# ğŸ”¹ 4ï¸âƒ£ ì‹¤í–‰
# ============================================
if __name__ == "__main__":
    category_input = "ê°•ì˜ì •ë³´"
    query = "ì¸ê³µì§€ëŠ¥ ê´€ë ¨ ê°•ì˜ ì•Œë ¤ì¤˜ "

    print_top3_and_llm(category_input, query)
