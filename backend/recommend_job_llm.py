# ============================================
# ğŸ“š SBERT ê¸°ë°˜ ì§ì—… ì¶”ì²œ + LLM ì„¤ëª… ì‹œìŠ¤í…œ
#   - doc_chunks.category = 'ì§ì—…'
#   - ì§ì—… ì—¬ëŸ¬ ê°œ(top_k) ì¶”ì²œ + GPTë¡œ ì„¤ëª… ìƒì„±
# ============================================

import os
import psycopg2
from sentence_transformers import SentenceTransformer
from openai import OpenAI
from dotenv import load_dotenv

# ===== .env ë¶ˆëŸ¬ì˜¤ê¸° =====
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# ===== ê¸°ë³¸ ì„¤ì • =====
PG_DSN = {
    "host": "localhost",
    "dbname": "kwchatbot",
    "user": "postgres",
    "password": "3864"
}

SIM_THRESHOLD = 0.25  # í•„ìš”í•˜ë©´ ì¡°ì •

# ===== SBERT ëª¨ë¸ =====
MODEL_PATH = "jhgan/ko-sbert-sts"
model = SentenceTransformer(MODEL_PATH)

# ===== OpenAI í´ë¼ì´ì–¸íŠ¸ =====
client = OpenAI(api_key=OPENAI_API_KEY)


# ============================================
# ğŸ”¹ category='ì§ì—…' ì—ì„œ ìœ ì‚¬ ì²­í¬ ê²€ìƒ‰
# ============================================
def _fetch_similar_job_chunks(query_embedding, top_k=3):
    """
    'ì§ì—…' ì¹´í…Œê³ ë¦¬ì—ì„œ ì‚¬ìš©ì ì¿¼ë¦¬ì™€ ê°€ì¥ ìœ ì‚¬í•œ ì²­í¬ top_kê°œë¥¼ ì°¾ìŒ
    """
    conn = psycopg2.connect(**PG_DSN)
    cur = conn.cursor()

    cur.execute(
        """
        SELECT 
            dc.doc_id,
            dc.chunk_id,
            dc.chunk_text,
            dc.category,
            1 - (e.embedding <#> %s::vector) AS similarity
        FROM embeddings e
        JOIN doc_chunks dc ON e.chunk_id = dc.chunk_id
        WHERE dc.category = 'ì§ì—…'
        ORDER BY e.embedding <#> %s::vector
        LIMIT %s;
        """,
        (query_embedding, query_embedding, top_k)
    )

    rows = cur.fetchall()
    cur.close()
    conn.close()
    return rows


# ============================================
# ğŸ”¹ ì§ì—… ì¶”ì²œ + LLM ì„¤ëª…
# ============================================
def recommend_jobs_with_llm(user_query, top_k=3):
    """
    ì‚¬ìš©ìì˜ ê´€ì‹¬ì‚¬(ë¬¸ì¥)ë¥¼ ì…ë ¥ë°›ì•„,
    category='ì§ì—…'ì— í•´ë‹¹í•˜ëŠ” ì§ì—… ì •ë³´ë¥¼ top_kê°œ ì¶”ì²œí•˜ê³ ,
    ì¶”ì²œ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ GPTê°€ í•œê¸€ ì„¤ëª…ì„ ìƒì„±í•œë‹¤.

    ë°˜í™˜ ì˜ˆì‹œ:
    {
      "ì¶”ì²œ_ì§ì—…_ëª©ë¡": [
        {"doc_id": 1, "title": "ë°ì´í„° ë¶„ì„ê°€", "ìš”ì•½": "...", "ìœ ì‚¬ë„": 0.83},
        ...
      ],
      "LLM_ì„¤ëª…": "..."
    }
    """

    # 1ï¸âƒ£ ì‚¬ìš©ì ì¿¼ë¦¬ ì„ë² ë”©
    query_embedding = model.encode(user_query).tolist()

    # 2ï¸âƒ£ ì§ì—… ì²­í¬ ê²€ìƒ‰
    rows = _fetch_similar_job_chunks(query_embedding, top_k=top_k)

    if not rows:
        return "â— ê´€ë ¨ ì§ì—… ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

    job_results = []
    for doc_id, chunk_id, chunk_text, category, sim in rows:
        sim = float(sim)
        if sim < SIM_THRESHOLD:
            continue

        title = chunk_text.splitlines()[0].strip()  # ì²« ì¤„ì„ ì§ì—…ëª…ìœ¼ë¡œ ì‚¬ìš©

        job_results.append({
            "doc_id": doc_id,
            "title": title,
            "ìš”ì•½": chunk_text,
            "ìœ ì‚¬ë„": round(sim, 4),
        })

    if not job_results:
        return "â— ìœ ì‚¬ë„ ê¸°ì¤€ì„ ë„˜ëŠ” ì§ì—…ì´ ì—†ìŠµë‹ˆë‹¤. (SIM_THRESHOLD ì¡°ì • í•„ìš”)"

    # 3ï¸âƒ£ LLM í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    system_prompt = """
    ë‹¹ì‹ ì€ ê´‘ìš´ëŒ€í•™êµ í•™ìƒë“¤ì„ ìœ„í•œ ì§„ë¡œ ìƒë‹´ ë„ìš°ë¯¸ì…ë‹ˆë‹¤.
    ì•„ë˜ ì§ì—… ëª©ë¡ì€ í•™ìƒì˜ ê´€ì‹¬ì‚¬ì™€ ìœ ì‚¬í•œ ì§ì—…ë“¤ì…ë‹ˆë‹¤.
    ê° ì§ì—…ì´ ì–´ë–¤ ì¼ì„ í•˜ëŠ”ì§€, ì–´ë–¤ ì—­ëŸ‰ì´ í•„ìš”í•œì§€,
    ê·¸ë¦¬ê³  ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ì–´ë–»ê²Œ ì—°ê²°ë˜ëŠ”ì§€
    ëŒ€í•™ìƒ/ì·¨ì¤€ìƒ ëˆˆë†’ì´ì— ë§ì¶° ê°„ë‹¨í•˜ê³  ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•´ ì£¼ì„¸ìš”.

    - ë„ˆë¬´ ê¸¸ì§€ ì•Šê²Œ ì •ë¦¬í•˜ê³ 
    - ê° ì§ì—…ë³„ë¡œ í•µì‹¬ í¬ì¸íŠ¸ ìœ„ì£¼ë¡œ ì¨ ì£¼ì„¸ìš”.
    """

    jobs_text = "\n\n".join(
        [f"- {j['title']} (ìœ ì‚¬ë„: {j['ìœ ì‚¬ë„']})\n{j['ìš”ì•½']}" for j in job_results]
    )

    user_prompt = f"""
    [ì‚¬ìš©ì ì§ˆë¬¸]
    {user_query}

    [ì¶”ì²œ ì§ì—… ëª©ë¡]
    {jobs_text}
    """

    try:
        resp = client.chat.completions.create(
            model=OPENAI_MODEL,
            temperature=0.5,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
        )
        explanation = resp.choices[0].message.content
    except Exception as e:
        explanation = f"â— GPT ì„¤ëª… ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    return {
        "ì¶”ì²œ_ì§ì—…_ëª©ë¡": job_results,
        "LLM_ì„¤ëª…": explanation
    }


# ============================================
# ğŸ”¹ ì‹¤í–‰ ì˜ˆì‹œ
# ============================================
if __name__ == "__main__":
    query = "ì—‘ì…€ì´ë‚˜ ë°ì´í„°ë¥¼ ë‹¤ë£¨ëŠ” ì‚¬ë¬´ì§ ì¼ì„ í•˜ê³  ì‹¶ì–´ìš”"
    from pprint import pprint

    rec = recommend_jobs_with_llm(query, top_k=3)
    print(f"[ì‚¬ìš©ì ì…ë ¥] {query}\n")
    pprint(rec)
